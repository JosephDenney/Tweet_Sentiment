{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA and Preprocessing Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\josep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\josep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\josep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer, TfidfTransformer\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<WordListCorpusReader in '.../corpora/stopwords' (not loaded yet)>\n",
      "{'regarding', 'seeming', 'whole', 'yours', 'then', 'have', 'thereby', 'thru', 'for', 'whoever', 'becoming', 'though', 'next', 'toward', 'everyone', 'now', 're', 'somewhere', 'become', 'and', 'every', 'latterly', 'thereupon', 'just', 'yourself', 'cannot', 'with', 'towards', 'would', 'another', 'here', 'hereafter', 'since', 'he', 'its', 'therefore', 'how', 'rather', 'at', 'are', 'done', 'whereafter', 'over', 'below', 'take', 'whom', 'nine', 'nowhere', 'around', 'off', 'doing', 'amongst', 'eleven', 'call', 'meanwhile', 'not', 'his', 'if', 'an', 'used', 'beyond', 'throughout', 'many', 'upon', 'most', 'down', 'ours', 'otherwise', 'keep', 'nothing', 'beside', 'top', 'made', 'to', 'being', 'up', 'whereupon', 'a', 'further', 'get', 'once', 'twenty', 'mine', 'no', 'been', 'me', 'six', 'anyhow', 'had', 'does', 'indeed', 'into', 'or', 'they', 'under', 'somehow', 'very', 'besides', 'amount', 'although', 'hence', 'less', 'may', 'whence', 'by', 'becomes', 'go', 'who', 'someone', 'there', 'anywhere', 'except', 'empty', 'afterwards', 'is', 'above', 'back', 'before', 'much', 'in', 'front', 'noone', 'until', 'also', 'part', 'it', 'namely', 'anyone', 'two', 'still', 'those', 'of', 'might', 'more', 'yet', 'onto', 'perhaps', 'thereafter', 'some', 'per', 'always', 'third', 'twelve', 'least', 'already', 'seems', 'via', 'too', 'own', 'him', 'during', 'using', 'among', 'fifteen', 'must', 'nor', 'unless', 'three', 'ten', 'whereas', 'seemed', 'anything', 'their', 'moreover', 'name', 'my', 'seem', 'sometimes', 'out', 'enough', 'so', 'because', 'serious', 'whither', 'ever', 'nevertheless', 'only', 'ca', 'show', 'where', 'after', 'us', 'various', 'her', 'bottom', 'say', 'whenever', 'hundred', 'side', 'else', 'hereby', 'therein', 'please', 'full', 'few', 'than', 'fifty', 'were', 'hereupon', 'make', 'quite', 'was', 'others', 'whereby', 'i', 'which', 'something', 'almost', 'herself', 'that', 'this', 'whatever', 'thence', 'any', 'as', 'wherein', 'four', 'never', 'ourselves', 'one', 'became', 'could', 'beforehand', 'either', 'put', 'whether', 'together', 'while', 'within', 'same', 'when', 'such', 'both', 'anyway', 'everywhere', 'these', 'what', 'really', 'other', 'none', 'itself', 'several', 'along', 'alone', 'be', 'five', 'sixty', 'move', 'why', 'forty', 'has', 'herein', 'mostly', 'wherever', 'however', 'about', 'whose', 'sometime', 'against', 'eight', 'our', 'everything', 'across', 'again', 'elsewhere', 'hers', 'should', 'through', 'from', 'even', 'your', 'neither', 'nobody', 'himself', 'she', 'behind', 'but', 'last', 'did', 'them', 'former', 'you', 'the', 'thus', 'can', 'see', 'all', 'do', 'give', 'formerly', 'myself', 'on', 'between', 'we', 'am', 'without', 'often', 'due', 'themselves', 'latter', 'each', 'well', 'yourselves', 'will', 'first'}\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/product_tweets.csv',encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative emotion', 'Positive emotion',\n",
       "       'No emotion toward brand or product', \"I can't tell\"], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_there_an_emotion_directed_at_a_brand_or_product'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns= {'is_there_an_emotion_directed_at_a_brand_or_product'\n",
    "                         :'Emotion','emotion_in_tweet_is_directed_at': 'Platform'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns= {'tweet_text': 'Tweet'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet            Platform  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "            Emotion  \n",
       "0  Negative emotion  \n",
       "1  Positive emotion  \n",
       "2  Positive emotion  \n",
       "3  Negative emotion  \n",
       "4  Positive emotion  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() # want to remove the @'name' in the tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummify = pd.get_dummies(df['Emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I can't tell</th>\n",
       "      <th>Negative emotion</th>\n",
       "      <th>No emotion toward brand or product</th>\n",
       "      <th>Positive emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   I can't tell  Negative emotion  No emotion toward brand or product  \\\n",
       "0             0                 1                                   0   \n",
       "1             0                 0                                   0   \n",
       "2             0                 0                                   0   \n",
       "3             0                 1                                   0   \n",
       "4             0                 0                                   0   \n",
       "\n",
       "   Positive emotion  \n",
       "0                 0  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 0  \n",
       "4                 1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dummify.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I can't tell                           156\n",
       "Negative emotion                       570\n",
       "No emotion toward brand or product    5389\n",
       "Positive emotion                      2978\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dummify.sum() # class bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      "Tweet       9092 non-null object\n",
      "Platform    3291 non-null object\n",
      "Emotion     9093 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "df = pd.merge(df, df_dummify, how='outer',on=df.index) # ran this code, dummify emotion data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9093 entries, 0 to 9092\n",
      "Data columns (total 8 columns):\n",
      "key_0                                 9093 non-null int64\n",
      "Tweet                                 9092 non-null object\n",
      "Platform                              3291 non-null object\n",
      "Emotion                               9093 non-null object\n",
      "I can't tell                          9093 non-null uint8\n",
      "Negative emotion                      9093 non-null uint8\n",
      "No emotion toward brand or product    9093 non-null uint8\n",
      "Positive emotion                      9093 non-null uint8\n",
      "dtypes: int64(1), object(3), uint8(4)\n",
      "memory usage: 390.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_0</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>I can't tell</th>\n",
       "      <th>Negative emotion</th>\n",
       "      <th>No emotion toward brand or product</th>\n",
       "      <th>Positive emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   key_0                                              Tweet  \\\n",
       "0      0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1      1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2      2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3      3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4      4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "             Platform           Emotion  I can't tell  Negative emotion  \\\n",
       "0              iPhone  Negative emotion             0                 1   \n",
       "1  iPad or iPhone App  Positive emotion             0                 0   \n",
       "2                iPad  Positive emotion             0                 0   \n",
       "3  iPad or iPhone App  Negative emotion             0                 1   \n",
       "4              Google  Positive emotion             0                 0   \n",
       "\n",
       "   No emotion toward brand or product  Positive emotion  \n",
       "0                                   0                 0  \n",
       "1                                   0                 1  \n",
       "2                                   0                 1  \n",
       "3                                   0                 0  \n",
       "4                                   0                 1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns = {\"I can't tell\": \"Uncertain\", 'Negative emotion': 'Negative'\n",
    "                          , 'No emotion toward brand or product': 'No Emotion'\n",
    "                          , 'Positive emotion':'Positive'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Uncertain</th>\n",
       "      <th>Negative</th>\n",
       "      <th>No Emotion</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet            Platform  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "            Emotion  Uncertain  Negative  No Emotion  Positive  \n",
       "0  Negative emotion          0         1           0         0  \n",
       "1  Positive emotion          0         0           0         1  \n",
       "2  Positive emotion          0         0           0         1  \n",
       "3  Negative emotion          0         1           0         0  \n",
       "4  Positive emotion          0         0           0         1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns='key_0')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.',\n",
       " \"@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW\",\n",
       " '@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.',\n",
       " \"@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw\",\n",
       " \"@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)\",\n",
       " '@teachntech00 New iPad Apps For #SpeechTherapy And Communication Are Showcased At The #SXSW Conference http://ht.ly/49n4M #iear #edchat #asd',\n",
       " nan,\n",
       " '#SXSW is just starting, #CTIA is around the corner and #googleio is only a hop skip and a jump from there, good time to be an #android fan',\n",
       " 'Beautifully smart and simple idea RT @madebymany @thenextweb wrote about our #hollergram iPad app for #sxsw! http://bit.ly/ieaVOB',\n",
       " 'Counting down the days to #sxsw plus strong Canadian dollar means stock up on Apple gear']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = list(df['Tweet'])\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenz = word_tokenize(','.join(str(v) for v in corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', '@', 'wesley83', 'I', 'have', 'a', '3G', 'iPhone', '.', 'After']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenz[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Stopwords List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_list = list(nlp.Defaults.stop_words)\n",
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['regarding',\n",
       " 'seeming',\n",
       " 'whole',\n",
       " 'yours',\n",
       " 'then',\n",
       " 'have',\n",
       " 'thereby',\n",
       " 'thru',\n",
       " 'for',\n",
       " 'whoever',\n",
       " 'becoming',\n",
       " 'though',\n",
       " 'next',\n",
       " 'toward',\n",
       " 'everyone',\n",
       " 'now',\n",
       " 're',\n",
       " 'somewhere',\n",
       " 'become',\n",
       " 'and',\n",
       " 'every',\n",
       " 'latterly',\n",
       " 'thereupon',\n",
       " 'just',\n",
       " 'yourself',\n",
       " 'cannot',\n",
       " 'with',\n",
       " 'towards',\n",
       " 'would',\n",
       " 'another',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'since',\n",
       " 'he',\n",
       " 'its',\n",
       " 'therefore',\n",
       " 'how',\n",
       " 'rather',\n",
       " 'at',\n",
       " 'are',\n",
       " 'done',\n",
       " 'whereafter',\n",
       " 'over',\n",
       " 'below',\n",
       " 'take',\n",
       " 'whom',\n",
       " 'nine',\n",
       " 'nowhere',\n",
       " 'around',\n",
       " 'off',\n",
       " 'doing',\n",
       " 'amongst',\n",
       " 'eleven',\n",
       " 'call',\n",
       " 'meanwhile',\n",
       " 'not',\n",
       " 'his',\n",
       " 'if',\n",
       " 'an',\n",
       " 'used',\n",
       " 'beyond',\n",
       " 'throughout',\n",
       " 'many',\n",
       " 'upon',\n",
       " 'most',\n",
       " 'down',\n",
       " 'ours',\n",
       " 'otherwise',\n",
       " 'keep',\n",
       " 'nothing',\n",
       " 'beside',\n",
       " 'top',\n",
       " 'made',\n",
       " 'to',\n",
       " 'being',\n",
       " 'up',\n",
       " 'whereupon',\n",
       " 'a',\n",
       " 'further',\n",
       " 'get',\n",
       " 'once',\n",
       " 'twenty',\n",
       " 'mine',\n",
       " 'no',\n",
       " 'been',\n",
       " 'me',\n",
       " 'six',\n",
       " 'anyhow',\n",
       " 'had',\n",
       " 'does',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'or',\n",
       " 'they',\n",
       " 'under',\n",
       " 'somehow',\n",
       " 'very',\n",
       " 'besides',\n",
       " 'amount',\n",
       " 'although',\n",
       " 'hence',\n",
       " 'less',\n",
       " 'may',\n",
       " 'whence',\n",
       " 'by',\n",
       " 'becomes',\n",
       " 'go',\n",
       " 'who',\n",
       " 'someone',\n",
       " 'there',\n",
       " 'anywhere',\n",
       " 'except',\n",
       " 'empty',\n",
       " 'afterwards',\n",
       " 'is',\n",
       " 'above',\n",
       " 'back',\n",
       " 'before',\n",
       " 'much',\n",
       " 'in',\n",
       " 'front',\n",
       " 'noone',\n",
       " 'until',\n",
       " 'also',\n",
       " 'part',\n",
       " 'it',\n",
       " 'namely',\n",
       " 'anyone',\n",
       " 'two',\n",
       " 'still',\n",
       " 'those',\n",
       " 'of',\n",
       " 'might',\n",
       " 'more',\n",
       " 'yet',\n",
       " 'onto',\n",
       " 'perhaps',\n",
       " 'thereafter',\n",
       " 'some',\n",
       " 'per',\n",
       " 'always',\n",
       " 'third',\n",
       " 'twelve',\n",
       " 'least',\n",
       " 'already',\n",
       " 'seems',\n",
       " 'via',\n",
       " 'too',\n",
       " 'own',\n",
       " 'him',\n",
       " 'during',\n",
       " 'using',\n",
       " 'among',\n",
       " 'fifteen',\n",
       " 'must',\n",
       " 'nor',\n",
       " 'unless',\n",
       " 'three',\n",
       " 'ten',\n",
       " 'whereas',\n",
       " 'seemed',\n",
       " 'anything',\n",
       " 'their',\n",
       " 'moreover',\n",
       " 'name',\n",
       " 'my',\n",
       " 'seem',\n",
       " 'sometimes',\n",
       " 'out',\n",
       " 'enough',\n",
       " 'so',\n",
       " 'because',\n",
       " 'serious',\n",
       " 'whither',\n",
       " 'ever',\n",
       " 'nevertheless',\n",
       " 'only',\n",
       " 'ca',\n",
       " 'show',\n",
       " 'where',\n",
       " 'after',\n",
       " 'us',\n",
       " 'various',\n",
       " 'her',\n",
       " 'bottom',\n",
       " 'say',\n",
       " 'whenever',\n",
       " 'hundred',\n",
       " 'side',\n",
       " 'else',\n",
       " 'hereby',\n",
       " 'therein',\n",
       " 'please',\n",
       " 'full',\n",
       " 'few',\n",
       " 'than',\n",
       " 'fifty',\n",
       " 'were',\n",
       " 'hereupon',\n",
       " 'make',\n",
       " 'quite',\n",
       " 'was',\n",
       " 'others',\n",
       " 'whereby',\n",
       " 'i',\n",
       " 'which',\n",
       " 'something',\n",
       " 'almost',\n",
       " 'herself',\n",
       " 'that',\n",
       " 'this',\n",
       " 'whatever',\n",
       " 'thence',\n",
       " 'any',\n",
       " 'as',\n",
       " 'wherein',\n",
       " 'four',\n",
       " 'never',\n",
       " 'ourselves',\n",
       " 'one',\n",
       " 'became',\n",
       " 'could',\n",
       " 'beforehand',\n",
       " 'either',\n",
       " 'put',\n",
       " 'whether',\n",
       " 'together',\n",
       " 'while',\n",
       " 'within',\n",
       " 'same',\n",
       " 'when',\n",
       " 'such',\n",
       " 'both',\n",
       " 'anyway',\n",
       " 'everywhere',\n",
       " 'these',\n",
       " 'what',\n",
       " 'really',\n",
       " 'other',\n",
       " 'none',\n",
       " 'itself',\n",
       " 'several',\n",
       " 'along',\n",
       " 'alone',\n",
       " 'be',\n",
       " 'five',\n",
       " 'sixty',\n",
       " 'move',\n",
       " 'why',\n",
       " 'forty',\n",
       " 'has',\n",
       " 'herein',\n",
       " 'mostly',\n",
       " 'wherever',\n",
       " 'however',\n",
       " 'about',\n",
       " 'whose',\n",
       " 'sometime',\n",
       " 'against',\n",
       " 'eight',\n",
       " 'our',\n",
       " 'everything',\n",
       " 'across',\n",
       " 'again',\n",
       " 'elsewhere',\n",
       " 'hers',\n",
       " 'should',\n",
       " 'through',\n",
       " 'from',\n",
       " 'even',\n",
       " 'your',\n",
       " 'neither',\n",
       " 'nobody',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'behind',\n",
       " 'but',\n",
       " 'last',\n",
       " 'did',\n",
       " 'them',\n",
       " 'former',\n",
       " 'you',\n",
       " 'the',\n",
       " 'thus',\n",
       " 'can',\n",
       " 'see',\n",
       " 'all',\n",
       " 'do',\n",
       " 'give',\n",
       " 'formerly',\n",
       " 'myself',\n",
       " 'on',\n",
       " 'between',\n",
       " 'we',\n",
       " 'am',\n",
       " 'without',\n",
       " 'often',\n",
       " 'due',\n",
       " 'themselves',\n",
       " 'latter',\n",
       " 'each',\n",
       " 'well',\n",
       " 'yourselves',\n",
       " 'will',\n",
       " 'first']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list.extend(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list.extend(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "516"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"wouldn't\", '“', '”', '...', \"''\", '’', '``', 'https', 'rt', '\\\\.+']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_punc = ['“','”','...',\"''\",'’','``','https','rt','\\.+']\n",
    "stopword_list.extend(additional_punc)\n",
    "stopword_list[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords and additional punctuation from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopped_tokenz = [word.lower() for word in tokenz if word.lower() not in stopword_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sxsw', 9414),\n",
       " ('mention', 7120),\n",
       " ('link', 4313),\n",
       " ('google', 2592),\n",
       " ('ipad', 2431),\n",
       " ('apple', 2300),\n",
       " ('quot', 1696),\n",
       " ('iphone', 1513),\n",
       " ('store', 1469),\n",
       " (\"'s\", 1236),\n",
       " ('2', 1114),\n",
       " ('new', 1087),\n",
       " ('austin', 959),\n",
       " ('amp', 836),\n",
       " ('app', 810),\n",
       " ('launch', 653),\n",
       " ('circles', 651),\n",
       " ('social', 647),\n",
       " ('android', 574),\n",
       " ('today', 574),\n",
       " (\"n't\", 481),\n",
       " ('network', 465),\n",
       " ('ipad2', 457),\n",
       " ('pop-up', 420),\n",
       " ('line', 402),\n",
       " ('free', 387),\n",
       " ('called', 361),\n",
       " ('party', 346),\n",
       " ('sxswi', 340),\n",
       " ('mobile', 338),\n",
       " ('major', 301),\n",
       " ('like', 290),\n",
       " ('time', 271),\n",
       " (\"'re\", 265),\n",
       " ('temporary', 264),\n",
       " ('opening', 257),\n",
       " (\"'m\", 254),\n",
       " ('possibly', 240),\n",
       " ('people', 226),\n",
       " ('downtown', 225),\n",
       " ('apps', 224),\n",
       " ('great', 222),\n",
       " ('maps', 219),\n",
       " ('going', 217),\n",
       " ('check', 215),\n",
       " ('mayer', 214),\n",
       " ('day', 214),\n",
       " ('open', 210),\n",
       " ('popup', 209),\n",
       " ('need', 205)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = FreqDist(stopped_tokenz)\n",
    "freq.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize the Data and use Regex to find and remove URL's, Tags, other misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_misc = ['sxsw','mention',r'[a-zA-Z]+\\'?s]',r\"(http[s]?://\\w*\\.\\w*/+\\w+)\"\n",
    "                   ,r'\\#\\w*',r'RT [@]?\\w*:',r'\\@\\w*',r\"\\d$\",r\"^\\d\"\n",
    "                   ,r\"([a-zA-Z]+(?:'[a-z]+)?)\",r'\\d.',r'\\d','RT'] #[A-Z]{2,20} remove caps like MAGA and CDT\n",
    "stopword_list.extend(additional_misc)\n",
    "stopword_list.extend(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_stopped_tokenz = [word.lower() for word in stopped_tokenz if word not in stopword_list]\n",
    "clean_lemmatized_tokenz = [lemmatizer.lemmatize(word.lower()) for word in stopped_tokenz if word not in stopword_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_clean_lemma = FreqDist(clean_lemmatized_tokenz)\n",
    "freq_lemma = freq_clean_lemma.most_common(5000)\n",
    "freq_lemma2 = freq_clean_lemma.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_word_count = len(clean_lemmatized_tokenz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_word_count = sum(freq_clean_lemma.values()) # just a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('link', 4324) ---- 4.865 %\n",
      "('google', 2593) ---- 2.918 %\n",
      "('ipad', 2431) ---- 2.735 %\n",
      "('apple', 2303) ---- 2.591 %\n",
      "('quot', 1696) ---- 1.908 %\n",
      "('iphone', 1513) ---- 1.702 %\n",
      "('store', 1508) ---- 1.697 %\n",
      "(\"'s\", 1236) ---- 1.391 %\n",
      "('new', 1087) ---- 1.223 %\n",
      "('austin', 960) ---- 1.080 %\n",
      "('amp', 836) ---- 0.941 %\n",
      "('app', 810) ---- 0.911 %\n",
      "('launch', 691) ---- 0.777 %\n",
      "('circle', 666) ---- 0.749 %\n",
      "('social', 647) ---- 0.728 %\n",
      "('android', 574) ---- 0.646 %\n",
      "('today', 574) ---- 0.646 %\n",
      "(\"n't\", 481) ---- 0.541 %\n",
      "('network', 473) ---- 0.532 %\n",
      "('ipad2', 457) ---- 0.514 %\n",
      "('line', 439) ---- 0.494 %\n",
      "('pop-up', 422) ---- 0.475 %\n",
      "('free', 387) ---- 0.435 %\n",
      "('party', 386) ---- 0.434 %\n",
      "('called', 361) ---- 0.406 %\n"
     ]
    }
   ],
   "source": [
    "for word in freq_lemma2:\n",
    "    normalized_freq = word[1] / lemma_word_count\n",
    "    print(word, \"----\", \"{:.3f}\".format(normalized_freq*100),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordcloud import WordCloud\n",
    "\n",
    "# ## Initalize a WordCloud with our stopwords_list and no bigrams\n",
    "# wordcloud = WordCloud(stopwords=stopword_list,collocations=False)\n",
    "\n",
    "# ## Generate wordcloud from stopped_tokens\n",
    "# wordcloud.generate(','.join(clean_lemmatized_tokenz))\n",
    "\n",
    "# ## Plot with matplotlib\n",
    "# plt.figure(figsize = (12, 12), facecolor = None) \n",
    "# plt.imshow(wordcloud) \n",
    "# plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "tweet_finder = nltk.BigramCollocationFinder.from_words(clean_lemmatized_tokenz)\n",
    "tweets_scored = tweet_finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(apple, store)</td>\n",
       "      <td>0.006639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(social, network)</td>\n",
       "      <td>0.005131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(new, social)</td>\n",
       "      <td>0.004703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(link, google)</td>\n",
       "      <td>0.003769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(google, launch)</td>\n",
       "      <td>0.003736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(network, called)</td>\n",
       "      <td>0.003679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(called, circle)</td>\n",
       "      <td>0.003499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(today, link)</td>\n",
       "      <td>0.003342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(major, new)</td>\n",
       "      <td>0.003263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(iphone, app)</td>\n",
       "      <td>0.003241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word      Freq\n",
       "0     (apple, store)  0.006639\n",
       "1  (social, network)  0.005131\n",
       "2      (new, social)  0.004703\n",
       "3     (link, google)  0.003769\n",
       "4   (google, launch)  0.003736\n",
       "5  (network, called)  0.003679\n",
       "6   (called, circle)  0.003499\n",
       "7      (today, link)  0.003342\n",
       "8       (major, new)  0.003263\n",
       "9      (iphone, app)  0.003241"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tweets_scored, columns=[\"Word\",\"Freq\"]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_pmi_finder = nltk.BigramCollocationFinder.from_words(clean_lemmatized_tokenz)\n",
    "tweet_pmi_finder.apply_freq_filter(5)\n",
    "\n",
    "tweet_pmi_scored = tweet_pmi_finder.score_ngrams(bigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>PMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(û÷sxsw, goûª)</td>\n",
       "      <td>14.117562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(jc, penney)</td>\n",
       "      <td>13.854528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(knitted, staircase)</td>\n",
       "      <td>13.854528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(naomi, campbell)</td>\n",
       "      <td>13.854528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(parking, 5-10)</td>\n",
       "      <td>13.854528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(pauly, celebs)</td>\n",
       "      <td>13.854528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(98, accuracy)</td>\n",
       "      <td>13.632135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(aron, pilhofer)</td>\n",
       "      <td>13.632135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(charlie, sheen)</td>\n",
       "      <td>13.632135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(lynn, teo)</td>\n",
       "      <td>13.632135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Words        PMI\n",
       "0      (û÷sxsw, goûª)  14.117562\n",
       "1          (jc, penney)  13.854528\n",
       "2  (knitted, staircase)  13.854528\n",
       "3     (naomi, campbell)  13.854528\n",
       "4       (parking, 5-10)  13.854528\n",
       "5       (pauly, celebs)  13.854528\n",
       "6        (98, accuracy)  13.632135\n",
       "7      (aron, pilhofer)  13.632135\n",
       "8      (charlie, sheen)  13.632135\n",
       "9           (lynn, teo)  13.632135"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tweet_pmi_scored, columns=[\"Words\",\"PMI\"]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Cleaned_Tweets.csv)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the two sets, lemmatized and not lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4 y columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopword_list,decode_error='ignore')\n",
    "\n",
    "# Vectorize data and make X_train_tfidf and X_test_tfidf\n",
    "X_train_tfidf = vectorizer.fit_transform(clean_lemmatized_tokenz)\n",
    "# X_test_tfidf = vectorizer.transform(X_test)\n",
    "# X_train_tfidf#.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<88875x8846 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 85700 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_token = TweetTokenizer()\n",
    "\n",
    "# pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "\n",
    "# for sentence in list(df['Tweet']):\n",
    "#     for word in str(sentence):\n",
    "#         word_tokenize(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = regexp_tokenize(','.join(str(v) for v in corpus), pattern=r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(stopped_tokenz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "tweet_finder = nltk.BigramCollocationFinder.from_words(stopped_tokenz)\n",
    "tweets_scored = tweet_finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tweets_scored, columns=['Words','Freq']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation, numbers, uppercase letters, tags, #hashtags, stopwords, and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a function that takes a dataframe column input and cleans that column\n",
    "# def clean_column(i=(0,len(corpus)-1)):\n",
    "#     from nltk.corpus import stopwords\n",
    "#     import string\n",
    "#     from nltk import word_tokenize,regexp_tokenize\n",
    "    \n",
    "#     print(f\"- Tweet #{i}:\\n\")\n",
    "#     print(corpus[i],'\\n')\n",
    "#     tokens = word_tokenize(corpus[i])\n",
    "\n",
    "#     # Get all the stop words in the English language\n",
    "#     stopwords_list = stopwords.words('english')\n",
    "#     stopwords_list += list(nlp.Defaults.stop_words)\n",
    "#     stopwords_list += string.punctuation\n",
    "#     stopwords_list += additional_punc\n",
    "#     stopped_tokenz = [w.lower() for w in tokenz if w.lower() not in stopwords_list]\n",
    "    \n",
    "#     print(tokenz,end='\\n\\n')\n",
    "#     print(stopped_tokenz)\n",
    "                \n",
    "# # run function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_column(df['Tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(','.join(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
